# Rapport - Semaine du 02/10/2017

* Récupération des missing pageviews (données envoyées à Morgane).
* Mise en place d'un serveur initial pour calcul/stockage.
* Développement d'un script créeant la file de traitement des pages wikipedia dans une MongoDB depuis les fichiers CSV de Morgane (même fichiers que pour indexer les données du datascape).
* Développement d'un programme de récupération en masse de pages wikipedia sur le web.
* Estimation de la taille de stockage nécessaire pour le HTML des pages récupérées (compressé pour gagner de l'espace): ~400mo pour 10k pages, minimum (juste le HTML). Prévoir donc > 200Go de stockage.
* Estimation du temps nécessaire: ~900 pages/min (~2.5 jours estimés pour la collecte totale).
* Revue de l'estimation en temps à la baisse à cause des limites de Wikipedia et pour respecter leurs serveurs: au-dessus de ~300 pages/min, ils commencent à s'inquiéter. Cependant, cette limite fonctionne par langue. Donc, en parallélisant la collecte par langue, on peut monter à ~600 pages/min (estimation basse, cela remonte parfois vers ~1000 pages/min, dépendant des serveurs de wikipedia et de l'entropie de l'ordre des langues dans la base).
* Début de la collecte sur un tiers des pages pour tester la scalabilité et fiabilité du script de collecte (1M).
* Fin de la collecte d'1M de pages et debut de la collecte d'1-2M de plus durant le week end. Début de la conception du script d'extraction des liens (parallélisé pour accélérer le calcul) dès lundi.
