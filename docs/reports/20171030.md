# Rapport - Semaine du 30/10/2017 (jeudi-vendredi)

Fin de la période de travail du mois d'Octobre.

* Affinage du script de résolution des composantes connexes d'alias de page location (heuristique basée sur de nombreux critères de score).
* Fin de l'écriture du script de création des nouveaux fichiers d'indexation pour le datascape (locations et trajectoires).
* Adaptation du script de création de l'index ElasticSearch.
* Evolution du datascape:
  - Intégration des nouvelles données (locations/trajectoires).
  - Ajout des trajectoires dans la page people.
  - Ajout de métadonnées dans la page location.
  - Ajout d'une page "meta" concernant les valeurs distinctes d'entités.

Le code des scripts ainsi que leur documentation est accessible sur le repo Github dans le dossier `mining`, à l'adresse suivante:

https://github.com/medialab/bhht-scripts/tree/master/mining

Le code du datascape est toujours accessible sur le repo privé Github dont l'adresse est la suivante:

https://github.com/medialab/bhht-datascape

## Point sur les données disponibles

Sur les bases MongoDB, voici les données dont je dispose (~90Go):

* Le HTML compressé de toutes les pages people.
* Les liens de toutes les pages people extraits depuis le HTML.
* Le HTML compressé de toutes les pages location.
* Les métadonnées wikidata pour toutes les pages location.
* Les labels de toutes les entités référencées par les métadonnées wikidata des pages location.

## Améliorations du datascape

* On voit désormais les Top 100 locations pour les dates négatives.
* La page people affiche correctement la succession de liens avec des locations.
* J'ai ajouté les aliases, les entités liées aux page locations.
* J'ai ajouté une page affichant la liste des entités dont les pages locations sont des instances.

Ayant besoin de la DSI pour la mise à jour et le redémarrage physique de certaines machines, je devrais pouvoir mettre en préprod puis en prod en debut de semaine prochaine.

## Remarques

* Je remarque que les pages de sportifs ont souvent une forte tendance à avoir énormément de liens vers des pages locations. Cela se comprend assez bien sur cette page: https://en.wikipedia.org/wiki/Leander_Paes . En effet, les sportifs ont souvent des tableaux récapitulatif de leur participation à des compétitions etc. Cela est vrai aussi pour les chanteurs/artistes mais dans une moindre mesure. Il faudrait décider si il convient de conserver les liens présents dans ces tableaux ou non.
* Malgré mes heuristiques d'extraction des liens, on ne peut éviter certains déboires. (Exemple: Louis XVI et France2 présent dans le corps du texte en fin d'article ou le United States Capitol présent dans les corps du texte de nombreuses page people de personnage d'avant JC, e.g. Hammurabi). Je décris une piste pour éviter cela dans la section suivante.
* N'ayant pas eu encore le fichier de Morgane réalisant l'union des lieux de naissance/décès et ceux présents dans les trajectoires, mon heuristique de liaison des composantes connexes d'alias a des lacunes et ne fonctionne pas au mieux de sa forme.

## Pour aller plus loin

### 1) Travail sur les instances

J'ai extrait les entités dont les pages locations sont des instance, selon wikidata. Ces entités ne sont ni "propres" ni très organisées/harmonisées.

Par exemple, les catégories "city", "city of France", "large city" existent et ne sont pas forcément cohérente ni hiérarchiques. Cependant, étant présentes pour de nombreuses pages location (> 50% min.) ces entités représentent une mine d'or pour constituer une typologie plus utile à vos besoins.

Concernant le label choisi, j'ai juste pris les labels dans toutes les langues et appliqué un ordre de priorité considérant le nombre de pages dans chaque langue. Ceci explique que certains labels soient en anglais (la majorité), et d'autres dans les autres langues.

Je vous enjoins donc à vous servir de la page dédiée sur le datascape afin de voir ce qu'il est possible d'agréger etc. pour définir une catégorisation (qu'il sera possible dans le futur d'utiliser comme base pour l'utilisation d'algorithmes de classification supervisée).

Si cela ne marchait pas, je vous recommande chaudement de considérer des algorithmes de topic modeling (e.g. LDA) afin de constituer une typologie autrement.

### 2) Affinage de l'extraction des liens

#### a) Pertinence des lieux dans le temps

Une manière "relativement" simple d'améliorer l'extraction des liens serait de calculer l'étendue temporelle des pages locations.

L'idée est de parser le HTML des pages locations pour y trouver des dates et en garder l'étendue (date min, date max).

Ensuite, lors de l'extraction des liens d'une page people, il suffirait de ne garder que les lieux ayant une étendue pertinente avec la vie de la personne.

#### b) Datation fine des points de contact

Il est possible d'affiner la datation des points de contact. Cela exige cependant de faire un peu de traitement automatique du langage (TAL). Il est nécessaire de disposer d'un tokenizer de phrases robuste et pouvant marcher avec le contenu Wikipedia qui n'est pas un contenu textual "parfait".

Certains système à base de règles doivent pouvoir fonctionner correctement. Le cas échéant il faudrait entraîner un Punkt (algorithme non-supervisé) pour chacune des langues considérées.

### 3) Affinage de la fusion des alias

La fusion des alias n'est pas parfaite et conserve quelques bugs (notamment "Sparta" & "Kansas Speedway"). En améliorant le script et la méthode de scoring (+ le fichier de Morgane), je pense qu'il est possible de règler une majeure partie de ses soucis.
