# Rapport - Semaine du 23/10/2017

* Développement d'un script scalable pour l'extraction des données de wikidata.
* Début de la récupération des données wikidata pour les 1M pages location. Rythme: 1k pages/min => estimation en temps: ~16h.
* Création d'un script de collection des entités wikidata uniques (wikidata c'est du web sémantique... donc rien n'est simple) qu'il va falloir aller chercher par la suite sur les bases de données de wikipedia (exemple: quand wikidata dit Amsterdam est en Hollande, il ne le dit pas directement; il dit Amsterdam est lié à Q345, il faut donc ré-interroger wikidata pour savoir ce qu'est Q345...).
* Ajustement de l'extraction des liens pour générer des listes ordonnées plutôt que des ensembles de lieux uniques.
* Documentation des scripts python.
* Crawl des 90k entités trouvées dans les informations wikidata concernant les pages people => estimatin en temps: ~2h.
* Nouvelle extraction des liens pour conserver l'ordre et utiliser des heuristiques sommaires de datation (~13h de calcul).
* Creation d'un script de dump des nouveaux fichiers nécessaires pour les locations & trajectoires en vue de la mise à jour du datascape.
* Adaptation du script de création de l'index ElasticSearch pour prendre en compte les nouvelles données enrichies.
* Test d'insertion des nouvelles données dans l'index ElasticSearch (~1h pour chaque test).
